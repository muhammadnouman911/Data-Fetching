{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Code For Indeed Jobs Web Scraping <h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"}\n",
    "url = \"https://www.indeed.com/jobs?q=software+developer&start={}\"\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "Scraping page 2...\n",
      "Scraping page 3...\n",
      "Scraping page 4...\n",
      "Scraping page 5...\n"
     ]
    }
   ],
   "source": [
    "for page in range(0, 50, 10): \n",
    "    print(f\"Scraping page {page // 10 + 1}...\")\n",
    "    url = url.format(page)\n",
    "    response = requests.get(url)\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    \n",
    "    job_containers = soup.find_all('div', class_='job_seen_beacon')\n",
    "    for job in job_containers:\n",
    "        try:\n",
    "            \n",
    "            job_title = job.find('h2', class_='jobTitle').text.strip()\n",
    "            \n",
    "           \n",
    "            company_name = job.find('span', class_='companyName').text.strip()\n",
    "            \n",
    "            \n",
    "            location = job.find('div', class_='companyLocation').text.strip()\n",
    "            \n",
    "            \n",
    "            salary = job.find('div', class_='metadata salary-snippet-container')\n",
    "            salary = salary.text.strip() if salary else \"Not Listed\"\n",
    "            \n",
    "          \n",
    "            job_summary = job.find('div', class_='job-snippet').text.strip()\n",
    "            \n",
    "         \n",
    "            jobs.append({\n",
    "                \"Job Title\": job_title,\n",
    "                \"Company Name\": company_name,\n",
    "                \"Location\": location,\n",
    "                \"Salary\": salary,\n",
    "                \"Job Summary\": job_summary\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping job: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping completed. Dataset saved as 'indeed_jobs_dataset.csv'.\n"
     ]
    }
   ],
   "source": [
    "df.to_csv(\"indeed_dataset.csv\", index=False)\n",
    "print(\"Scraping completed. Dataset saved as 'indeed_jobs_dataset.csv'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>This project focused on scraping job listings from Indeed using Python libraries like requests and BeautifulSoup. The aim was to extract information such as job titles, company names, locations, salaries, and summaries and save it to a CSV file. However, the project faced challenges due to Indeed's advanced bot detection, which blocked the requests and served a \"Blocked\" page. Additionally, the dynamic nature of the website, which relies on JavaScript to load content, made it difficult to extract the required data using simple scraping methods. While the project provided valuable learning about web scraping, overcoming these challenges would require using tools like Selenium or paid APIs for dynamic content handling.<h5>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
